{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcfae67c-f841-46d0-aa1e-544dc0caee98",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "[Chi tiết](https://scikit-learn.org/stable/modules/sgd.html#sgd)\n",
    "\n",
    "Linear fitting model (SVM, logistic regression, linear regression, etc.) (config by `loss` parameter) with SGD training: tối thiểu gradient (độ dốc) hàm loss liên tục qua mỗi batch liên tục (mặc định SGD __batchsize__ = Online SGD batchsize = 1, Mini-batch SGD batchsize > 1 but < n_samples)\n",
    "\n",
    "- Sử dụng hàm loss và fit lần lượt dữ liệu để tối thiểu hoá hàm convex loss thông qua gradient descent (đạo hàm)\n",
    "- SGD fit minibatch với method `partial_fit` (chỉ có ở 1 số algorithms) cho phép học online hoặc out-of-core learning (học tăng cường/cải thiện hơn so với model trước đó)\n",
    "- Nên sử dụng `StandardScaler` engineering trước khi fit\n",
    "- Phù hợp với dữ xliệu lớn > 10000 obs\n",
    "\n",
    "**Stopping criterion**\n",
    "- With `early_stopping`=`True`: input data được chia thành train + validation. Model  sẽ fit train set và tiêu chí dừng lại dựa vào prediction score của validation set. size của validation set theo param `validation_fraction`\n",
    "- With `early_stopping`=`False`: model sẽ fit toàn bộ input data và điều kiện dừng phụ thuộc vào objective funciton được tính toán trên tập train\n",
    "\n",
    "Trong cả hai TH của `early_stopping`, hàm loss đều được tính toán lại sau mỗi 1 epoch, điều kiện dừng là khi 1 trong 2 TH:\n",
    "- hàm loss mới có xu hướng tăng lên so với best loss (the min loss) 1 giá trị lớn hơn `tol` trong vòng `n_iter_no_change` epoch liên tiếp (tức là loss > best_loss + `tol`)\n",
    "- số epoch chạm ngưỡng `max_iter`\n",
    "\n",
    "**Pros:**\n",
    "- Hiệu quả nếu tunning chính xác và tỉ mẩn\n",
    "- Dễ dể thực hiện và áp dụng thực tế do có cơ chế `partial_fit`\n",
    "\n",
    "**Cons:**\n",
    "- SGD yêu cầu tunning nhiều parameters.\n",
    "- SGD nhạy cảm với feature scale (need to `StandardScaler`).\n",
    "- Do việc học từng phần có thêm phần chi phí lưu trữ nên chỉ sử dụng khi dữ liệu thực sự lớn và chia theo chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a138e737-0807-45c9-bef9-e4c6165e98e6",
   "metadata": {},
   "source": [
    "## SGD Regression\n",
    "\n",
    "**`loss` parameter**\n",
    "- loss=\"`squared_error`\": Ordinary least squares,\n",
    "- loss=\"`huber`\": Huber loss for robust regression,\n",
    "- loss=\"`epsilon_insensitive`\": linear Support Vector Regression. (linearSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f337a49-225f-4257-b16f-0948fc60f4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9494588558075366"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "reg = SGDRegressor(loss=\"squared_error\")\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# R-squared score\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "979f3078-95ed-41ce-a5cb-20ae4caffac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.950614663855244"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partial_fit\n",
    "reg.partial_fit(X_test, y_test)\n",
    "\n",
    "# R-squared score\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc77ea3-f0ba-48c1-8fef-951c1928eaa0",
   "metadata": {},
   "source": [
    "## SGD Classification\n",
    "\n",
    "**`loss` parameter**\n",
    "- ‘`hinge`’ gives a linear SVM.\n",
    "- ‘`log_loss`’ gives logistic regression, a probabilistic classifier.\n",
    "- ‘`modified_huber`’ is another smooth loss that brings tolerance to outliers as well as probability estimates.\n",
    "- ‘`squared_hinge`’ is like hinge but is quadratically penalized.\n",
    "- ‘`perceptron`’ is the linear loss used by the perceptron algorithm.\n",
    "- The other losses, ‘`squared_error`’, ‘`huber`’, ‘`epsilon_insensitive`’ and ‘`squared_epsilon_insensitive`’ are designed for regression but can be useful in classification as well; see `SGDRegressor` for a description.\n",
    "\n",
    "**`learning_rate`**\n",
    "- ‘constant’: eta = eta0\n",
    "- ‘optimal’: eta = 1.0 / (alpha * (t + t0)) where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
    "- ‘invscaling’: eta = eta0 / pow(t, power_t)\n",
    "- ‘adaptive’: eta = eta0, miễn là loss tiếp tục theo chiều hướng giảm. Nếu trong vòng `n_iter_no_change` epoch mà loss tăng vượt quá `tol` hoặc validation score bị giảm quá `tol`( trong TH set `early_stopping`=`True`) thì eta = eta/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fcf71643-1767-44fb-aa53-9d13c1c00f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_cl, y_cl = make_classification(n_samples=2000, n_features=20, n_informative=10, n_redundant=5, n_classes=2,flip_y = 0.1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cl, y_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2b70bd4e-8f79-49b7-85c7-841c8bf090a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.684"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge', # type of loss function\n",
    "                    penalty='l2', # type of penalty to coef, L1 dẫn tới phần lớn các coef = 0, elasticnet cân bằng l1 + l2\n",
    "                    alpha=0.0001, # hệ số nhân với penalty, càng cao thì regularization càng lớn\n",
    "                    l1_ratio=0.15, # hệ số cân bằng l1 với l2 khi sử dụng elasticnet regularization\n",
    "                    fit_intercept=True, # có estimate intercept hay không ?\n",
    "                    max_iter=1000, # số each tối đa cho fitting (chỉ áp dụng cho fit, ko áp dụng cho partial_fit)\n",
    "                    tol=0.001, # chêch lệch tối đa cho phép giữa loss hiện tại và best_loss (the min loss), nếu vượt thì sẽ dừng lại fitting\n",
    "                    shuffle=True, # data có được shuffle sau mỗi 1 each được train\n",
    "                    verbose=0, \n",
    "                    epsilon=0.1, # chỉ ảnh hưởng tới hàm loss của ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.\n",
    "                                    # đối với 'huber' là ngưỡng mà tại đó những dự đoán chính xác trở nên ít quan tâm, chỉ care những dụ đoán sai gây ra nhiều loss\n",
    "                                    # đối với ‘epsilon_insensitive’ là ngưỡng mà khi sự khác biệt giũa prediction và real mà nhỏ hơn ngưỡng này thì bỏ qua\n",
    "                    n_jobs=None, \n",
    "                    random_state=None, # nếu không cố định, SGD performance bị ảnh hưởng rất nhiều bởi điểm khởi đầu\n",
    "                    learning_rate='optimal', # xem tại note learning_rate\n",
    "                    eta0=0.0, # learning_rate ban đầu\n",
    "                    power_t=0.5, # hệ số chỉnh khi set learning_rate = 'invscaling'\n",
    "                    early_stopping=False,  # có split ra validation set và xét early stopping trên đó hay không ?(stop khi validation score is not improved)\n",
    "                    validation_fraction=0.1, # tỷ lệ phân chia cho validation set\n",
    "                    n_iter_no_change=5, # số lượng epoch cho phép loss/validation score không được cải thiện\n",
    "                    class_weight=None, # đánh trọng số cho tường class, dùng trong TH imbalance data\n",
    "                    warm_start=False, # nếu True, sử dụng các tham số đã có của lần call fit trước đó làm các giá trị ban đầu, nếu ko thì xét lại từ đầu\n",
    "                    average=False # coef được tính average weight cho các lần cập nhật hay ko, nếu True là all, nếu = 10 thì lấy average coef_ của 10 lần update gần nhất\n",
    "                   )\n",
    "\n",
    "clf = make_pipeline(StandardScaler(),sgd)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}