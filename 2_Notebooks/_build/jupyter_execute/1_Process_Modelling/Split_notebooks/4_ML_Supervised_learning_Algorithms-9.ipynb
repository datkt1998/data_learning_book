{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33e26d71-a2f4-4cb7-9e18-ab4615d269d8",
   "metadata": {},
   "source": [
    "# Neural network models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c70c6e8-3b8c-44bb-9113-2b93709e3fa4",
   "metadata": {},
   "source": [
    "## Extreme Learning Machine (ELM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeadf470-e5d1-4ac1-9ba8-ad2f241eebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ELMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, node, activation_func, output_node='softmax', faster=None, C=None, p=None, random_state=None):\n",
    "        self.no_node = node\n",
    "        \n",
    "        self.af = activation_func\n",
    "        if self.af not in ['relu','sig','sin','hardlim','new']:\n",
    "            raise ValueError('Parameter Activation function must be one in [relu, sig, sin, hardlim, new]')\n",
    "            \n",
    "        self.output_node = output_node\n",
    "        if self.output_node not in ['sig','softmax']:\n",
    "            raise ValueError('Parameter Activation function must be one in [relu, sig, sin, hardlim, new]')\n",
    "        \n",
    "        self.faster = faster\n",
    "        if self.faster not in [None,1,2]:\n",
    "            raise ValueError('Parameter faster must be one in [None, 1, 2]')\n",
    "        \n",
    "        self.C = C\n",
    "        if self.faster != None:\n",
    "            if C == None: \n",
    "                raise ValueError('Missing regularization value C')\n",
    "        \n",
    "        self.p = p\n",
    "        if (self.af == 'new'): \n",
    "            if p == None:\n",
    "                raise ValueError('Missing p for new activation function')\n",
    "            else:\n",
    "                self.p = int(p)\n",
    "\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Check the unique classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "        self.y_ = OneHotEncoder().fit_transform(y.reshape(-1,1)).toarray()\n",
    "        \n",
    "        # Initiate random input weights and bias\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        self.input_weights = np.random.normal(size=[X.shape[1],self.no_node])\n",
    "        self.biases = np.random.normal(size=[self.no_node])\n",
    "        \n",
    "        # The smallest norm least square solution\n",
    "        H = self.hidden_nodes(X)\n",
    "        if self.faster == 1:\n",
    "            self.output_weights = H.T @ np.linalg.inv(np.identity(X.shape[0])/self.C + H @ H.T) @ self.y_\n",
    "        elif self.faster == 2:\n",
    "            self.output_weights = np.linalg.inv(np.identity(self.no_node)/self.C + H.T @ H) @ H.T @ self.y_\n",
    "        else:\n",
    "            self.output_weights = np.dot(np.linalg.pinv(H), self.y_)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0, x)\n",
    "    \n",
    "    def sin(self, x):\n",
    "        return np.sin(x)\n",
    "    \n",
    "    def hardlim(self, x):\n",
    "        x = np.array(x)\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return expit(x) # 1/(1+np.exp(-x)) RuntimeWarning: overflow encountered in exp\n",
    "    \n",
    "    def new(self, x, p):\n",
    "        return x/(1 + x**(4*p-2))**(1/(4*p-2))\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x)/np.sum(np.exp(x), 1).reshape(-1,1)\n",
    "    \n",
    "        \n",
    "    def hidden_nodes(self, X):\n",
    "        G = np.dot(X, self.input_weights) + self.biases\n",
    "        if self.af == 'relu':\n",
    "            H = self.relu(G)\n",
    "        elif self.af == 'sig':\n",
    "            H = self.sigmoid(G)\n",
    "        elif self.af == 'sin':\n",
    "            H = self.sin(G)\n",
    "        elif self.af == 'hardlim':\n",
    "            H = self.hardlim(G)\n",
    "        elif self.af == 'new':\n",
    "            H = self.new(G)\n",
    "            \n",
    "        return H\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        prediction = np.argmax(np.dot(self.hidden_nodes(X), self.output_weights),1)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def predict_proba(self, X, mode='softmax'):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        prediction = np.dot(self.hidden_nodes(X), self.output_weights)\n",
    "        if self.output_node == 'softmax':\n",
    "            proba = self.softmax(prediction) # Use softmax function to convert to probability, calculating for different classes' output\n",
    "        elif self.output_node == 'sig':\n",
    "            proba = self.sigmoid(prediction[:,1]) # Use sigmoind function to convert to probability from a real value (for binary classification ONLY, output just for positive class\n",
    "  \n",
    "        return proba\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {'node':self.no_node,\n",
    "                'activation_func':self.af,\n",
    "                'faster':'self.faster', \n",
    "                'C':self.C, \n",
    "                'p':self.p, \n",
    "                'random_state':self.random_state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5267a4-6528-47f9-893b-c4666f4b08c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EELMClassifier(ELMClassifier):\n",
    "    def __init__(self, node, activation_func, output_node='softmax', faster=None, C=None, p=None, no_bat=20, alpha=0.5, gamma=0.5, max_iter=100, freq_min=0, freq_max=10):\n",
    "        super().__init__(node, activation_func, output_node, faster, C, p)\n",
    "        \n",
    "        self.bat_pop = no_bat # Population of bats\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # Parameters for random walk\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Range of Frequency\n",
    "        self.freq_min = freq_min\n",
    "        self.freq_max = freq_max\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Check the unique classes seen during fit\n",
    "        self.X_ = X\n",
    "        self.classes_ = unique_labels(y)\n",
    "        self.y_ = OneHotEncoder().fit_transform(y.reshape(-1,1)).toarray()\n",
    "        \n",
    "        # Initialization parameter for BA\n",
    "        self.dim = self.no_node*X.shape[1] + self.no_node # Dimension of Pos\n",
    "        self.freq = np.zeros(self.bat_pop)\n",
    "         \n",
    "        self.Pos = np.random.rand(self.bat_pop,self.dim) # Store Positions: each row is Pos of a bat\n",
    "        self.Vel = np.random.rand(self.bat_pop,self.dim) # Store Velocity: each row is Vel of a bat for the defined dimension\n",
    "        self.L = np.random.rand(self.bat_pop) # Initiate Loudness for each bat\n",
    "        self.r_init = np.random.rand(self.bat_pop) # Initiate Pulse emission rate for each bat\n",
    "        self.r = self.r_init\n",
    "        \n",
    "        # Store the best solution\n",
    "        self.fitness = np.zeros(shape=(self.max_iter, self.bat_pop)) # Store fitness for each bat's Pos as row\n",
    "        self.best = np.zeros(shape=(self.max_iter, self.dim)) # Store the best Pos that minimizes the fitness as row\n",
    "        self.f_min = np.zeros(self.max_iter) # Store fitness value for the best Pos\n",
    "        \n",
    "        self.final_Pos, self.final_rmse = self.generate_pos()\n",
    "        \n",
    "        self.input_weights = self.final_Pos[:-self.no_node].reshape(X.shape[1], self.no_node)\n",
    "        self.biases = self.final_Pos[-self.no_node:]\n",
    "        \n",
    "        # The smallest norm least square solution\n",
    "        H = self.hidden_nodes(X)\n",
    "        if self.faster == 1:\n",
    "            self.output_weights = H.T @ np.linalg.inv(np.identity(X.shape[0])/self.C + H @ H.T) @ self.y_\n",
    "        elif self.faster == 2:\n",
    "            self.output_weights = np.linalg.inv(np.identity(self.no_node)/self.C + H.T @ H) @ H.T @ self.y_\n",
    "        else:\n",
    "            self.output_weights = np.dot(np.linalg.pinv(H), self.y_)\n",
    "    \n",
    "    def best_bat(self,t):\n",
    "        # t: t_th iteration\n",
    "        best_bat = np.argmin(self.fitness[t]) # Get the bat with best fitness\n",
    "        self.best[t] = self.Pos[best_bat] # Store its Pos\n",
    "        self.f_min[t] = self.fitness[t, best_bat] # Store its fitness value\n",
    "     \n",
    "    def generate_pos(self):\n",
    "        t = 0\n",
    "        while t < self.max_iter:\n",
    "            for i in range(self.bat_pop):\n",
    "                r = np.random.rand() # Random value from a uniform distribution over [0, 1)\n",
    "                self.freq[i] = self.freq_min + (self.freq_max - self.freq_min)*r\n",
    "                self.Vel[i] = self.Vel[i] + (self.Pos[i] - self.best[t])*self.freq[i]\n",
    "                self.Pos[i] = self.Pos[i] + self.Vel[i]\n",
    "                # Update fitness\n",
    "                self.fitness[t, i] = self.rmse(self.Pos[i], 1)          \n",
    "            \n",
    "            # Get the best solution\n",
    "            self.best_bat(t)\n",
    "            \n",
    "            # Random walk procedure\n",
    "            epsilon = np.random.uniform(-1, 1)\n",
    "            for i in range(len(self.L)):\n",
    "                self.Pos[i] = self.Pos[i] + epsilon*self.L[i]\n",
    "            \n",
    "            self.L = self.alpha*self.L\n",
    "            self.r = self.r_init*(1-np.exp(-self.gamma*t))\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        return self.best[np.argmin(self.f_min)], np.min(self.f_min)\n",
    "    \n",
    "    def rmse(self, Pos, random_state):\n",
    "        # Split train:test = 9:1\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X_, self.y_, test_size=0.1, stratify=self.y_, random_state=random_state)\n",
    "        \n",
    "        input_weights = Pos[:-self.no_node].reshape(X_train.shape[1], self.no_node)\n",
    "        biases = Pos[-self.no_node:]\n",
    "        \n",
    "        #train\n",
    "        G = np.dot(X_train, input_weights) + biases\n",
    "        if self.af == 'relu':\n",
    "            H = self.relu(G)\n",
    "        elif self.af == 'sig':\n",
    "            H = self.sigmoid(G)\n",
    "        elif self.af == 'sin':\n",
    "            H = self.sin(G)\n",
    "        elif self.af == 'hardlim':\n",
    "            H = self.hardlim(G)\n",
    "        elif self.af == 'new':\n",
    "            H = self.new(G)\n",
    "        \n",
    "        if self.faster == 1:\n",
    "            output_weights = H.T @ np.linalg.inv(np.identity(X.shape[0])/self.C + H @ H.T) @ y_train\n",
    "        elif self.faster == 2:\n",
    "            output_weights = np.linalg.inv(np.identity(self.no_node)/self.C + H.T @ H) @ H.T @ y_train\n",
    "        else:\n",
    "            output_weights = np.dot(np.linalg.pinv(H), y_train)\n",
    "        \n",
    "        #validate\n",
    "        G_val = np.dot(X_test, input_weights) + biases\n",
    "        if self.af == 'relu':\n",
    "            H_val = self.relu(G_val)\n",
    "        elif self.af == 'sig':\n",
    "            H_val = self.sigmoid(G_val)\n",
    "        elif self.af == 'sin':\n",
    "            H_val = self.sin(G_val)\n",
    "        elif self.af == 'hardlim':\n",
    "            H = self.hardlim(G_val)\n",
    "        elif self.af == 'new':\n",
    "            H_val = self.new(G_val)\n",
    "        \n",
    "        T_hat = np.dot(H_val, output_weights)\n",
    "        \n",
    "        return np.linalg.norm(T_hat - y_test)\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {'node':self.no_node,\n",
    "                'activation_func':self.af,\n",
    "                'faster':'self.faster', \n",
    "                'C':self.C, \n",
    "                'p':self.p, \n",
    "                'no_bat':self.bat_pop, \n",
    "                'alpha':self.alpha, \n",
    "                'gamma':self.gamma, \n",
    "                'max_iter':self.max_iter, \n",
    "                'freq_min':self.freq_min, \n",
    "                'freq_max':self.freq_max}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4b76559-4fe3-4dfb-9236-831e63bdaf69",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron (MLP)\n",
    "\n",
    "Sử dụng kiến trúc mạng neural network nhiều hidden layer với input m chiều (m node + 1 bias node tại input layer) và output là o chiều (o node tại outpput layer), áp dụng cho model linear & non-linear classification và regression \n",
    "<img src=\"https://i.ibb.co/vw8GWsy/MLP.jpg\" width=\"500\"> \n",
    "_Note_: NN in sklearn not support GPU trainning\n",
    "\n",
    "__Pros__:\n",
    "- Capability to learn non-linear models.\n",
    "- Capability to learn models in real-time (on-line learning) using `partial_fit`.\n",
    "\n",
    "__Cons__:\n",
    "- MLP với hidden layers có hàm a non-convex loss thì có thể có nhiều hơn 1 local minimum. Dẫn tới việc có thể có sự sai khác trong quá trình training tại các điểm startpoint ban đầu khác nhau\n",
    "- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "- MLP is sensitive to feature scaling.\n",
    "\n",
    "**1. Regularization**\n",
    "\n",
    "Both `MLPRegressor` and `MLPClassifier` use parameter $\\alpha$ for regularization (L2 regularization) term which helps in avoiding overfitting by penalizing weights with large magnitudes.\n",
    "\n",
    "**2. Hidden layer size** `hidden_layer_sizes`\n",
    "- Số hidden layers\n",
    "- Số node mỗi hidden layer \n",
    " \n",
    "|<img src=\"https://qph.fs.quoracdn.net/main-qimg-65fa680a5effca84096237df3eb4ae88\" width =400>|\n",
    "|:--:|\n",
    "\n",
    "- In `sklearn.neural_network.MLPClassifier`\n",
    "    - `hidden_layer_sizes` = (9): 1 hidden layer với 9 node\n",
    "    - `hidden_layer_sizes` = (9, 7, 4): 3 hidden layers với lần lượt 9,7 and 4 node\n",
    "    - default (100,):  1 hidden layer với 100 neurons/node\n",
    "\n",
    "`clf = MLPClassifier(hidden_layer_sizes=(9, 9,9))`\n",
    "\n",
    "**3. Algorithms** (`solver`)\n",
    "\n",
    "Các phương pháp update weight:\n",
    "\n",
    "- `sgd` | __SGD__(supports online and mini-batch learning): weight được update theo hàm loss và learning rate\n",
    "$$w\\leftarrow w-\\eta(\\alpha\\frac{\\partial R(w)}{\\partial w}+\\frac{\\partial L o s s}{\\partial w})$$\n",
    "__SGD__ with momentum or nesterov’s momentum, on the other hand, can perform better than those two below algorithms if `learning rate` is correctly tuned.\n",
    "\n",
    "- `adam`__Adam__(supports online and mini-batch learning)(default): Adam kết hợp sự thay đổi momentum dựa theo gradient bậc 1 và bậc 2 . __Adam__ is very robust for relatively large datasets\n",
    "\n",
    "- `lbfgs` | __L-BFGS__(not supports online and mini-batch learning) is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. __L-BFGS__ converges faster and with better solutions on small datasets. Thuật toán sử dụng phương pháp quasi-Newton methods để tìm nghiệm của điểm tối ưu\n",
    "\n",
    "**4. Activation functions** (`activation`)\n",
    "\n",
    "- `identity`: simply returns $f(x) = x$\n",
    "- `logistic`: the logistic sigmoid function, returns $f(x) = 1 / (1 + \\exp(-x))$\n",
    "- `tanh`: the hyperbolic tan function, returns $f(x) = \\tanh(x)$\n",
    "- `relu` (default):   the rectified linear unit function, returns $f(x) = \\max(0, x)$\n",
    "\n",
    "**5. Learning Rate**\n",
    "- Sklearn supports __learning rates__  when solver=`sgd`\n",
    "    - `constant`: a constant,  given by `learning_rate_init=0.001`.\n",
    "    - `invscaling`: Giảm từ từ sau mỗi 1 epoch train\n",
    "    - `adaptive`: \n",
    "\n",
    "**6. Tips**\n",
    "- highly recommended to scale your data\n",
    "- Find best $\\alpha$ in range $[10^{-6}, 10^{-1}]$ by gridsearch\n",
    "- If you want more control over stopping criteria or learning rate in __SGD__, or want to do additional monitoring, using `warm_start`=True and `max_iter`=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "073d29db-9ac3-45d2-81a0-beae072b73fa",
   "metadata": {},
   "source": [
    "### MLP Classification\n",
    "Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a28fa575-31a2-40ae-a12a-abeafe822ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "edf85e88-0f2b-4e9c-ad6f-46feb44c9fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data and preprocessing\n",
    "def prepro_data():\n",
    "    # load data\n",
    "    df = pd.read_csv('Datasets/adult.csv' ).replace(\"?\", np.nan)\n",
    "\n",
    "    # X and y\n",
    "    y = df['income'].apply(lambda x:0 if x=='<=50K' else 1)\n",
    "    X = df.drop(['income'],axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "    # preprocessing\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('ohe', OneHotEncoder(handle_unknown= 'infrequent_if_exist'), make_column_selector(dtype_exclude=np.number)),\n",
    "        ('scaler', MinMaxScaler(), make_column_selector(dtype_include=np.number))\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    X_train = preprocessor.fit_transform(X_train, y_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepro_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f7251b3c-0b62-4277-99d2-cd79dcd0f70b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8605793837649708"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup estimator\n",
    "clf = MLPClassifier(random_state=1,  max_iter=3000)\n",
    "\n",
    "parameter_grid = {\n",
    "    'hidden_layer_sizes': [(3), # 1 hidden layer with 3 nodes   \n",
    "                           (3,3) # 2 hidden layer with 3 nodes each layer\n",
    "                          ], \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(clf, parameter_grid,  cv=5)\n",
    "gs = gs.fit(X_train,y_train)\n",
    "\n",
    "#set the clf to the best combination of parameters\n",
    "clf_best = gs.best_estimator_\n",
    "print(\"best model:\", clf_best.get_params())\n",
    "# Fit the best model to the data. \n",
    "clf_best = clf_best.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(y_test, clf_best.predict(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4592d1e-1693-4d33-b0f3-6bc71571872b",
   "metadata": {},
   "source": [
    "### MLP Regression\n",
    "Class `MLPRegressor` trains sử dụng __backpropagation__ with no activation function in the output layer, mà tại đó có thể sử dụng hàm customize để đạt được output mong muốn. Vì thế nó sử dụng the square error như là loss function, và the output là giá trị continuous.\n",
    "\n",
    "MLPRegressor also supports multi-output regression, in which a sample can have more than one target."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}